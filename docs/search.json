[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Physicians: Learning with Ease & Efficiency",
    "section": "",
    "text": "Preface\n\n\n\nFigure 1: An adventure map\n\n\nWelcome to your essential guide to learning R for medical professionals. If you’re a physician eager to master R, you may find yourself at a crossroads. On the one hand, you have the vast online universe of random tutorials, StackOverflow threads, and YouTube videos. On the other, you have highly-reviewed online courses that promise a comprehensive understanding of R but often don’t cater to your specific needs as a medical professional.\nThis book is different.\nAs a physician who ventured into the world of R post-residency (and now uses it daily), I found myself mired in information overload. Online resources, while immensely helpful, were scattered and disjointed. Online courses, on the other hand, offered structure but lacked the specific context that physicians need. This gap is what led me to pen this guide - a streamlined, context-rich resource tailor-made for physicians.\nThis guide is built on real-life experiences and hurdles that I faced in my journey of learning R. The issues that confused me, the challenges that stumped me, the “aha” moments I had - they’re all in here. But more than that, this guide anticipates and addresses the likely issues that you, as a physician learning R, will encounter.\nEach chapter is a step further in your journey, with examples and use-cases centered around medical scenarios, making the learning experience highly relevant and practical for you. Whether it’s data manipulation, statistical analysis, or data visualization, the examples and use-cases are drawn from medical scenarios to make your learning experience as relevant as possible.\nEmbrace this guide as an ally in your pursuit of R mastery. It’s a journey that’s already been tread, with roadblocks cleared and signposts erected for easy navigation. This book isn’t about wandering aimlessly through tutorials, it’s about embarking on a well-blazed trail, specifically charted for physicians by a physician. So welcome aboard, let’s turn the page and commence our shared adventure."
  },
  {
    "objectID": "getting_started.html#what-is-r",
    "href": "getting_started.html#what-is-r",
    "title": "1  Getting Started",
    "section": "1.1 What is R?",
    "text": "1.1 What is R?\nFigure 1.1 AI-generated art capturing R statistics in the abstract\nR is a free, open-source programming language and software environment specifically designed for statistical computing and graphics. Ross Ihaka and Robert Gentleman created R at the University of Auckland and it is now maintained by the R Development Core Team. R is essentially composed of a core language and a variety of user interfaces. The core language, often referred to as “base R”, is where all the computation and processing happens. The user interfaces, such as RStudio, provide an intuitive frontend where users write code, visualize data, and manage their workflows.\nThe idea for R came about when Drs. Ross Ihaka and Robert Gentleman were teaching an introductory statistics course and were unsatisfied with the statistical software available to them. They wanted to create a software that was free, user-friendly, and provided an effective way to teach their students statistics.\nInterestingly, R is named partly after the first names of the two R authors (Robert and Ross) and partly as a play on the name of S, an influential statistical programming language at the time.\nSince its creation, R has grown exponentially, with a vibrant community of users and developers from various fields like academia, industry, and data science. It’s maintained by a large, global group of volunteers who continually add to its capabilities by creating new packages."
  },
  {
    "objectID": "getting_started.html#why-r",
    "href": "getting_started.html#why-r",
    "title": "1  Getting Started",
    "section": "1.2 Why R?",
    "text": "1.2 Why R?\nFor those of us who’ve engaged with statistical analysis during our education or careers, the memory of installing a hard-to-get software replete with countless menus, allowing interaction primarily through mouse clicks, is all too familiar. Perhaps you ran various commands on data, or dove into different analyses, all while navigating through these menus and outputs. I recall those times vividly and suspect many of you have similar experiences.\nR, to many, might initially appear as an obscure programming language tucked away in academia or tech-based industries.\nBut let me challenge your perception!\nThe very traits of R that lend it an air of obscurity are, in fact, its most significant assets."
  },
  {
    "objectID": "getting_started.html#great-things-about-r",
    "href": "getting_started.html#great-things-about-r",
    "title": "1  Getting Started",
    "section": "1.3 Great things about R",
    "text": "1.3 Great things about R\n\nYou don’t have to install R:\nUnlike traditional software, R doesn’t demand space on your computer. It runs smoothly in the cloud, making it accessible from anywhere, on any device. More importantly, the scripts, the ‘statistical documents’ you write in R, are not one-off commands. They are reusable, editable, and shareable pieces of code that capture your entire analytical process from start to finish.\nR is free:\nI still have bad memories of trying to find a student copy of an expensive statistics software I bought in college. The complete and latest R suite is free to run on the platform of your choice and the 100% cloud-based R-Studio has a generous amount of resources for their free plan.\nActive and Friendly Community:\nR has a large, active, and helpful user community. This means help is often readily available through online forums, blogs, and tutorials.\nNarratives, not isolation:\nWriting scripts in R provides a natural and coherent flow to your work, a linear narrative, if you will. Instead of isolated tables and analyses separated by an output window, you have a comprehensive, logical story.\nAwesome outputs:\nThe table and graphics capabilities of R are second to none. Packages such as flextable and ggplot2 provide advanced functionality for creating high-quality, customizable, and publication-ready graphics. You will quickly recognize in print and media that R is everywhere!\n\nAfter spending some time with R, becoming familiar with its capabilities, and experiencing its versatility firsthand, you might find it hard to even recognize it compared to the statistical programs you used back in college."
  },
  {
    "objectID": "getting_started.html#using-rstudio-cloud",
    "href": "getting_started.html#using-rstudio-cloud",
    "title": "1  Getting Started",
    "section": "1.4 Using RStudio Cloud",
    "text": "1.4 Using RStudio Cloud\nRStudio Cloud is a great tool that simplifies the process of setting up R. It allows you to run R directly from your web browser, eliminating the need to install software locally and handle any setup hassles. In 2023, RStudio was renamed Posit Studio and Posit Cloud but in this book, I will continue to refer to it as RStudio and RCloud.\n\nCreating an Account\nFirst, navigate to the RStudio Cloud website (https://rstudio.cloud/). If you don’t have an account yet, click on “Sign Up” to create one. Enter your details, then click “Register”. You’ll receive an email to confirm your account.\n\n\n\nCreating a New Project\nAfter you’ve logged in, you’ll see your RStudio Cloud workspace. Click on the “New Project” button to start a new R project. Enter a name for your project and then click on “Create Project”.\n\n\n\nRStudio Cloud Interface\nNow you’re inside the RStudio interface, running within your web browser. On the left, you’ll see the R console where you can enter R commands. The right panel contains tabs for plots, packages, help, and files. The top-left panel is for scripts or R Markdown files.\n\n\n\nWriting and Running R Code\nTo start coding, click on the “File” menu, then “New File”, and then “R Script”. An editor will open where you can write your R code. After writing your code, you can run it by clicking on the “Run” button, or by pressing Ctrl+Enter (Cmd+Enter on Mac).\n\n\n\nRun some practice code\nLet’s test out your setup by printing “Hello World!”. In your empty script type print(\"Hellow World!\")` and click on the “Run” button.\n\n\n\nSaving and Sharing Your Work\nRStudio Cloud autosaves your work as you go, so you don’t have to worry about losing your code. If you want to share your project, click on the “Settings” gear icon in the top-right corner of the project, and set “Who can view this project” to “Everyone”. You can then share the URL of your project with others.\n\n\n\n\nCongratulations! You’re now up and running with RStudio Cloud. You have a versatile, powerful tool at your fingertips, ready to tackle your data analysis needs."
  },
  {
    "objectID": "workshop.html#the-big-picture",
    "href": "workshop.html#the-big-picture",
    "title": "2  The R Workshop",
    "section": "2.1 The Big Picture",
    "text": "2.1 The Big Picture\nIn previous chapters, we learned what R is and why we would want to use it. We setup access to R though our web browser by signing up for a free RStudio account. Let’s move ahead to learning about R packages. R packages instantly give you access to a universe of tools and datasets."
  },
  {
    "objectID": "workshop.html#introduction",
    "href": "workshop.html#introduction",
    "title": "2  The R Workshop",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nThe medicaldata package is a collection of datasets that are relevant to medical research. It offers a robust collection of medical datasets extracted from a wide range of study designs, including randomized controlled trials, retrospective and prospective cohort studies, and case-control studies. These datasets encompass a diverse array of medical conditions and treatment approaches, providing rich opportunities for learning, exploration, and analysis.\nThe package contains over 19 datasets, covering a wide range of medical topics, including cancer, cardiovascular disease, diabetes, and mental health. One of the datasets, ‘strep_tb’, for example, is drawn from the groundbreaking 1948 trial of Streptomycin treatment for tuberculosis, the first modern randomized, placebo-controlled clinical trial. The datasets in the medicaldata package are all in a standard format, which makes them easy to use with R."
  },
  {
    "objectID": "workshop.html#r-packages",
    "href": "workshop.html#r-packages",
    "title": "2  The R Workshop",
    "section": "2.3 R Packages",
    "text": "2.3 R Packages\nHow do we get access to all of these interesting datasets?\nThe beauty of R lies in its simplicity and ease of access to a wealth of data and tools. Unlike traditional methods where you might have to navigate to a website, download files, and manually place them into specific directories, R simplifies this process immensely. One of the big advantages of R is that it provides the capability to access numerous tools and datasets directly from the command line using a single line of code."
  },
  {
    "objectID": "workshop.html#packages-are-toolboxes",
    "href": "workshop.html#packages-are-toolboxes",
    "title": "2  The R Workshop",
    "section": "2.4 Packages are Toolboxes",
    "text": "2.4 Packages are Toolboxes\nThink of R as large workshop with access to a main tool depot. with access to lots of specialized toolkits. In this workshop you are assigned a personal workbench. The toolboxes are designed and put together by different craftsmen, making them unique in the tools they contain. If you’ve identified a toolbox that you need for a specific project, you first have to bring that toolbox into your workbench. In fact, it would not be unusual to retrieve several toolkits depending on needs of your project.\nHowever, just lugging the toolboxes to your workbench doesn’t necessarily mean you can immediately use the tools they contains. If you want access to all your screwdrivers at once, you will need open a specialized screwdriver toolkit. On the other hand, you might only want a single screwdriver from a special toolkit. this is more than just keeping your workbench tidy, you also don’t want to have duplicates of similar tools around which may lead to confusion."
  },
  {
    "objectID": "workshop.html#breaking-down-the-workshop-analogy",
    "href": "workshop.html#breaking-down-the-workshop-analogy",
    "title": "2  The R Workshop",
    "section": "2.5 Breaking down the workshop analogy",
    "text": "2.5 Breaking down the workshop analogy\nLet’s connect this analogy with learning R\n\nThe workshop represents R and RStudio\nThe personal workbench is your R project\nThe toolboxes are R packages\nThe tool depot is the Comprehensive R Archive Network (CRAN) package repository (more on this later!)\nLugging the toolkit to your workbench is analogous to installing the package\nOpening the entire toolkit is adding it to your active R libraries\nSelecting a single tool from a toolkit is the same as using the :: operator on a package."
  },
  {
    "objectID": "workshop.html#software-repositories-in-r",
    "href": "workshop.html#software-repositories-in-r",
    "title": "2  The R Workshop",
    "section": "2.6 Software repositories in R",
    "text": "2.6 Software repositories in R\n\n2.6.1 CRAN (https://cran.r-project.org/)\nThe CRAN repository where officially approved and tested packages are stored.These packages are well-documented, reliable, and updated regularly. So, if you need a tool for a common task, you’re likely to find a toolbox containing it in the CRAN repository.\n\n\n2.6.2 Development repositories\nPlaces like Github and other code repositories offer exciting, cutting-edge tools that may not have made their way to the main CRAN depot yet. While the tools from these workshops can be highly useful, they also come with a word of caution as they may not be as thoroughly tested and documented as those in the CRAN depot. You may also you need to use the latest advancements with well-known packages that have not been updated on CRAN yet."
  },
  {
    "objectID": "workshop.html#walkthrough",
    "href": "workshop.html#walkthrough",
    "title": "2  The R Workshop",
    "section": "2.7 Walkthrough",
    "text": "2.7 Walkthrough\nLet’s walk through loading the medicaldata package which will provide the datasets we will use thoughout the rest of book.\n\n2.7.1 Setting the stage\n\nFirst create a new R project in R Cloud\n\n\n\n\nFigure 2.2: Posit/RStudio Cloud Homepage\n\n\n\nGive your project a name by clicking above the menu bar. Type in RforDoctors.\n\nCreate a new empty R script\n\n\n\n\nFigure 2.3: Adding a new file\n\n\n\nSave the File as chapter1.R\n\nType the following into your blank script:\n\n# Install the R package medicaldata\ninstall.packages(\"medicaldata\")\n\n# Load package into active library\nlibrary(medicaldata)\n\nAccess the Code menu and select Run All\n\n\n\n\nFigure 2.4: Running your code\n\n\n\nExamine the output in the console window.\n\n\n\n\nFigure 2.5: The R Console"
  },
  {
    "objectID": "workshop.html#reviewing-the-code",
    "href": "workshop.html#reviewing-the-code",
    "title": "2  The R Workshop",
    "section": "2.8 Reviewing the Code",
    "text": "2.8 Reviewing the Code\nDiving into the code we’ve written, several key points need to be highlighted.\n\nCode isn’t like regular writing. Forget paragraphs; each command you write stands alone on its own line.\nThe pound symbol # leads us to the next key point. Placing this at the start of a line tells R to gloss over this part when executing code. This is what we call a comment and it’s a handy way to leave notes for yourself and others.\nThe third point concerns functions, such as install.packages() and library(). Consider functions as time-saving shortcuts for complicated operations. They take inputs and give outputs.\nWhen you see parentheses associated with a term, think function. Whatever goes inside these parentheses are known as input parameters to the function.\nLook closely at the use of quotes around medicaldata in the install.packages line, but their absence in the library line. In R, quotes aren’t just punctuation, they serve a specific function which we’ll delve deeper into later."
  },
  {
    "objectID": "workshop.html#examining-the-output",
    "href": "workshop.html#examining-the-output",
    "title": "2  The R Workshop",
    "section": "2.9 Examining the output",
    "text": "2.9 Examining the output\nOur first interactive coding command produced some interesting output in the R console. Let’s take a moment to discuss the R console and understand what it just told us.\nThe R console is akin to a live conversation with R. When you type a command and hit enter, R listens, processes the request, and then speaks back to you. This “speech” is the output you see on your screen. Let’s look at the output from our script:\n\n\n\nFigure 2.6: Reviewing the console output\n\n\nNow, back to the output of our first command, install.packages(\"medicaldata\"). This command tells R to install the “medicaldata” package, a collection of ready-made functions and data. R takes this command, connects to a server, and starts to download the package. It provides us with live updates, telling us how large the package is (650 KB), and its download status.\nThe next few lines, * installing *binary* package ‘medicaldata’ ... and * DONE (medicaldata), tell us that R has successfully installed the package.\nThe last line of the output, ‘/tmp/Rtmpi7lTow/downloaded_packages’, is R’s way of saying “If you need the downloaded files, here’s where I’ve stored them”.\nOnce the installation is complete, we run library(medicaldata). This command tells R to open the toolbox of medicaldata and make its tools available for use. There’s no output after this command, which usually indicates that the command has run successfully and the package is ready to use."
  },
  {
    "objectID": "workshop.html#checking-the-results-of-our-work",
    "href": "workshop.html#checking-the-results-of-our-work",
    "title": "2  The R Workshop",
    "section": "2.10 Checking the results of our work",
    "text": "2.10 Checking the results of our work\nInstalling packages is a fundamental aspect of using R. In the next few chapters we will learn more about the language of R to manipulate and process data, but for now let’s see the fruit of our labor.\nMost R packages have excellent documentation. The medicaldata package is no exception. The guide to the datasets in the package can be found at this link: https://higgi13425.github.io/medicaldata/.\nLet’s use the instructions from the package author to view the different datasets we now have installed by typing into the console:\ndata(package = \"medicaldata\")\nOnce you execute this command, R will display an interactive window showing you all the datasets available in the “medicaldata” package. Each dataset is listed with a brief description of the kind of data it contains, which can be very useful when deciding which dataset to use for a particular analysis.\n\n\n\nFigure 2.7: Datasets within the medicaldata package"
  },
  {
    "objectID": "workshop.html#entering-data-within-a-script-versus-the-console",
    "href": "workshop.html#entering-data-within-a-script-versus-the-console",
    "title": "2  The R Workshop",
    "section": "2.11 Entering data within a script versus the console",
    "text": "2.11 Entering data within a script versus the console\nWhen you’re working with R, you have two main places where you can enter your data or commands: the script editor and the console.\n\n\n\nFigure 2.8: Navigating the RStudio Interface\n\n\nThe script editor is your workspace for crafting R scripts. This is where you write your lines of code, organize your thoughts, define functions, and generally create your R programs. Anything you write in the script editor is saved and can be run as many times as you want, making it ideal for larger, more complex analyses.\nOn the other hand, the console is the live interaction space where R executes commands and displays results. Anything you type directly into the console is run immediately, but it’s not saved once you close your R session. It’s a great place for quick calculations, testing small bits of code, or inspecting data.\nIn essence, the script editor is your drafting table where you design and plan, and the console is more like a chat where you can immediately execute and see your plans come to life. As you continue to work with R, you’ll become more comfortable determining when to use each for different tasks.\nIndeed, certain commands are best suited for direct execution in the console, especially those that serve to inspect data or check a package’s contents. The command to view the datasets within the medicaldata package is a good example of this. It’s a ‘single-use’ operation that doesn’t necessarily form part of the core workflow in your script, but rather provides you with valuable contextual information."
  },
  {
    "objectID": "workshop.html#chapter-summary",
    "href": "workshop.html#chapter-summary",
    "title": "2  The R Workshop",
    "section": "2.12 Chapter Summary",
    "text": "2.12 Chapter Summary\nIn this chapter, we introduced the basics of using R, with a specific focus on accessing and utilizing packages from CRAN. We introduced the concept of functions, explained the role of comments, and highlighted the differences between writing code in a script versus executing commands in the console. Through the installation and exploration of the ‘medicaldata’ package, we demonstrated the ease and power of working with packages in R. This foundational knowledge will serve as a solid base as we delve deeper into R programming in subsequent chapters."
  },
  {
    "objectID": "dataframes.html#the-big-picture-so-far",
    "href": "dataframes.html#the-big-picture-so-far",
    "title": "3  Data Frames",
    "section": "3.1 The big picture (so far)",
    "text": "3.1 The big picture (so far)\nNow that you’ve set up your rstudio.cloud account and familiarized yourself with the interface including the code editor and command console, it’s time to delve into the world of data frames. In the realm of medical practice, data frames serve as indispensable tools for organizing and analyzing clinical data."
  },
  {
    "objectID": "dataframes.html#what-are-data-frames",
    "href": "dataframes.html#what-are-data-frames",
    "title": "3  Data Frames",
    "section": "3.2 What are data frames?",
    "text": "3.2 What are data frames?\nData frames are the most widely used data structure in R and help you manage data in a structured, tabular format.\nData frames are similar in appearance to spreadsheets. It is a two-dimensional table where each column contains values of one variable, and each row contains one set of values from each column. A key thing to note about data frames is that different columns can contain different types of data (numeric, character, etc.), but each column must contain the same number of data items (rows)."
  },
  {
    "objectID": "dataframes.html#the-many-names-of-data-frames",
    "href": "dataframes.html#the-many-names-of-data-frames",
    "title": "3  Data Frames",
    "section": "3.3 The many names of data frames",
    "text": "3.3 The many names of data frames\nData frames in R come with an assortment of names, which can make the initial stages of learning R a bit confusing. Traditionally, a data frame in R is referred to as a ‘data.frame’. However, with the advent of packages like ‘tibble’ in the tidyverse ecosystem, a data frame can also be called a ‘tibble’. Meanwhile, in the data.table package, it’s called a ‘data.table’. Don’t let the various names confuse you; they all revolve around the same fundamental concept."
  },
  {
    "objectID": "dataframes.html#vignette-the-lunch-lecture-cram",
    "href": "dataframes.html#vignette-the-lunch-lecture-cram",
    "title": "3  Data Frames",
    "section": "3.4 Vignette: The Lunch Lecture Cram",
    "text": "3.4 Vignette: The Lunch Lecture Cram\n\n\n\nFigure 3.2: A captivating lunch lecture?\n\n\n\nYou are a 3rd year infectious disease fellow on a research block. Your supervisor has asked you to present a captivating lunch lecture to several visiting faculty. Knowing that many history buffs will be present, you decide to present a reanalysis of an infectious disease landmark - the discovery of streptomycin as a treatment for tuberculosis (TB). After all, why not? You have heard that the original dataset is only a few lines away in R! Fast forward a few weeks later. You break into a sweat realizing that the talk is only 24 hours away and you haven’t thought about it once since you were asked. However, your last few weeks’ priorities had other, undoubtedly urgent, plans. Here you are, with R as unfamiliar as before and the talk lurking just a day away. But hey, who needs calm preparation when you can have adrenaline-fueled learning marathons, right? Let the last-minute hustle begin! You sit down at your computer and open http://rstudio.cloud …\n\nWe can certainly empathize with the fellow’s predicament. Let’s imagine the questions racing through her head and see if we can help!\nWhere do I start?\nR differs from many statistical software packages where you typically start with your data. Instead, R operates more like a word processor. The ideal starting point is to open a new file, akin to launching a fresh document. Right after, it’s advisable to name and save the file promptly. Let’s designate ours “chapter2.R”.\n\nWhat do I type into my blank script?\nIt’s normal when starting R to find a blank script daunting. As you gain experience, this fear will fade and scripting will become intuitive. For now, starting with a template, much like you would in a word processor, can provide a supportive starting point, making the learning curve less steep.\nEnter the following:\n# Subject: Lunch Lecture on TB\n# Date: 6/7/2023\n# Author: Dr. Wall\n\n# Open the medicaldata toolbox\nlibrary(medicaldata)\n\n# View the available datasets\n# instructions at https://higgi13425.github.io/medicaldata/\ndata(package = \"medicaldata\")\n\n# Print the strep_tb dataset\nstrep_tb\nFigure 2.2: A template script to access the medicaldata package.\nHow do I “run” the code?\nLet’s jump right in - code, unlike a document, can be “run” or “executed” so the computer follows a set of instructions. The only catch is that the instructions must follow a certain set of rules or syntax.\nThe simplest way to run code in R is to execute the current line where your cursor is located. You can do this by pressing ‘Ctrl + Enter’ if you are using Windows/Linux or ‘Cmd + Enter’ if you are on a Mac. You can also click the “Run” button.\n\nWhat is the best strategy to run code?\nIt’s important to think of an R script as a story with a beginning, middle, and end. Though you can run sections in whatever order you want, however, it may not make any sense when R tries to execute the code.\nLet’s think about our current piece of code that consists of three parts:\n\n\nComments (Lines 1-3, 5, 8-9)\nAn instruction to open the medicaldata package (line 6)\nAn instruction to view all the data in the medicaldata package (line 10)\n\nSimilarly, when writing a script in R, certain instructions or packages must be initiated before others. If we consider an R package as a character in our story, you must first introduce (load) the character into the plot (the R environment) before they can take any actions (functions).\nAlso, if you modify any section of your code, you should plan on re-executing that section so R has an up to date understanding of what you want to do. Remember, executing your code regularly is a good practice as it allows you to catch and fix any errors or bugs as soon as they arise.\nWhat about comments?\nComments are the exception to the rule. Comments do NOT have to be executed. A properly formatted comments (i.e. starting with a pound symbol) is only interpreted by the human reading the code, it doesn’t have any particular meaning to R. So you can really place comments in what ever order you want.\nSo, can we see the dataset now?\nWe’re now ready to explore the dataset. Despite the initial learning process being somewhat tedious - stepping back - the process isn’t overly complicated. We’ve managed to carry out all tasks within a web browser, starting from an empty script and adding 2-3 lines of code. This has provided us with access to the specific data we’re interested in examining.\nThree Runs: Finally, let’s run some code!\nLet’s start by moving our cursor to line #1. This assures us that we are at the very beginning of the script and we don’t miss executing any lines of instructions. Let’s start by using either the hot key for running a line or clicking the “Run” button above the editor. We are going to do three consecutive runs and see what we can learn.\n\nThe First Run Line\nR will skip over each comment until it gets to the first instruction (Line 6). It will then execute the instruction. The cursor will end up on the comment on Line 8. You will notice new output in the console that prints the instruction that was run.\nIn this case, using the library function with the input of medicaldata did not result in any visible output. This is often referred to as silent output. If no warning or error messages are generated, this means the instruction was a success.\nFrom R’s perspective, we have now opened the medicaldata package and it is available for use. The Packages tab in the Files pane can confirm which packages are ready to use via a library function vs. packages that are generally available in your workspace.\n\nThe Second Run Line\n“Proceed with running the next line of code. As before, R will ignore comments and execute the data function instruction. However, this time, the result isn’t silent. A new tab should appear in your editor, presenting all the datasets housed within the medicaldata package.\nPlease scroll through this list until you find the Streptomycin/TB dataset that we’re interested in, and make sure to note down its name.”\nThis is a good example of how certain instructions may not be the best to leave within a script. In this case, trying to run this code as a single script may be interrupted by opening a new window. There is a very simple way, however, to keep track of this function without having to erase it. Just add # sign in front of it to comment it out. Even though it is a valid R instruction, the comment symbol will not let R see it as an instruction and it will not run. However, you will be able to know the command is still available and what purpose it served. If you need to run it again in the future, simply uncomment the line.\nIndeed, it’s important to note that not all instructions are optimal to include in a script that’s intended to be run as a whole. In this instance, if a piece of code triggers the opening of a new window, it could disrupt the execution flow of the script. However, there’s a straightforward solution to this: we can use the hashtag (#) to comment out this line of code.\nWhen you prepend a line of code with the # symbol, it effectively turns it into a comment. Despite being a valid R command, the interpreter will not execute it as such due to the comment symbol. This prevents the code from running, but it allows you to keep the command in your script for future reference or use.\nBy leaving the command as a commented line, you can quickly see what function it served and how it was used. If you need to execute this line in the future, all you need to do is remove the # sign to uncomment the line, making it a live command once more.\nThe Third Run Line\nProceed with the next line of code. Instead of a quiet output or triggering a new window, the console will start displaying a cascade of data table columns.\nWell done, this is your initial view of the strep_tb dataset!\n\nYou may find this partial view a bit limiting. Before learning R, many of us, including myself, primarily managed our data using software like Excel. Excel has its own challenges, but it undeniably offers an immediate and comprehensive view of all your data.\nDon’t fear! Much like my own experience, you’re likely to be pleasantly surprised by the vast array of powerful tools that R - including viewing the data as a spreadsheet. In the next chapter we will use these tools to get “x-ray” vision into our dataset."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "This book is designed as a guide, a companion on your journey to mastering R, but it’s not just another instruction manual. As a fellow physician who discovered the power of R after my residency, I’ve encountered and navigated the same challenges you’re likely to face. This book synthesizes those experiences into a streamlined learning path that addresses our unique needs and use-cases in the medical field. It is a comprehensive roadmap, crafted to transform you from an R novice to a confident user, capable of leveraging this powerful tool to improve your research, data analysis, and overall work.\nOur goal is not to become software engineers or statisticians, but rather to harness the potential of R to make our tasks more efficient and our decisions more data-driven. To this end, this book covers the essentials of R programming, data manipulation, statistical analysis, and data visualization. Moreover, the lessons are grounded in real-world, relatable examples, including using medical datasets, making the learning experience engaging and intuitive. By the end of this book, you will have gained a robust understanding of R, its application in healthcare, and most importantly, the confidence to explore further and ask the right questions of your data."
  },
  {
    "objectID": "rprogramming.html",
    "href": "rprogramming.html",
    "title": "5  Getting started with R programming",
    "section": "",
    "text": "6 What does programming in R entail?\nIf you’re familiar with office applications like Excel, then you already have experience with data manipulation and analysis, albeit in a different form. Programming in R is a logical step forward, providing a more sophisticated and powerful means to handle data. Imagine you’re working in Excel, but instead of clicking through menus and dialogs, you give direct written instructions about what you want to do. These instructions, which are lines of code in R, can range from simple tasks like importing data and calculating averages, to more complex ones like creating plots, running statistical tests, or building predictive models.\nR programming brings consistency, repeatability, and automation to the data analysis process. Suppose you’ve created a detailed report in Excel, and then receive a new set of data. You’d likely have to repeat a substantial amount of manual work. In contrast, with an R script, you could simply run the same script on the new data, saving substantial time and effort.\nProgramming might seem intimidating at first, especially if you’ve never done it before. However, with a bit of patience and practice, it becomes a powerful tool that can make your data work more efficient, effective, and even enjoyable.\nLets write some code to explore the Laryngoscope dataset found in the ‘medicaldata’ package. The Laryngoscope dataset contains data on 99 adult patients who required orotracheal intubation for elective surgery. The dataset includes patient demographics, airway assessment data, intubation success rate, time to intubation, ease of intubation, and occurrence of complications. This dataset is a valuable resource for researchers and clinicians who are interested in studying the factors that contribute to successful orotracheal intubation.\nThe Laryngoscope dataset serves as an excellent resource for us, containing a wide variety of data types. It provides an opportunity to explore and understand numerous fundamental programming functions in R.\nTo start working with the Laryngoscope dataset, you first need to load it into your R environment. The dataset is already included in the ‘medicaldata’ package, so if you have this package installed and loaded (as we did earlier), you can load the dataset with the following command:\ndata(“Laryngoscope”, package = “medicaldata”)\nThis command tells R to look for a dataset called “Laryngoscope” in the “medicaldata” package and load it into your current workspace.\nTo get an overview of the data, use the head() function, which shows the first six rows of the dataset:\nhead(Laryngoscope)\nTo get the structure of your data frame – like the number of rows, number of columns, and the type of data in each column – use the str() function:\nstr(Laryngoscope)\nThis will provide you an output indicating the class of each variable (numeric, integer, factor etc.), along with the first few entries of each.\nWe also can summarize the data using the summary() function:\nsummary(Laryngoscope)\nThe summary() function provides a statistical summary of the dataset, giving the minimum, 1st quartile, median, mean, 3rd quartile, and maximum for numerical columns, and the number of levels for categorical columns.\nEach of the variables in the dataset can be accessed using the $ operator. For example, to view the ‘Age’ column, you would use:\nLaryngoscope$Age\nThis will display all the age values for the patients in the dataset.\nUnderstanding data types is fundamental to successful programming and data analysis in R. Each data type has unique properties that dictate how it can be used and manipulated. Identifying the data type is often the first step you take when you’re working with new data. It allows you to understand the kind of information you are dealing with and how you can use it.\nFor example, numerical data types can undergo mathematical operations, while string (character) data types cannot. Logical data types store true/false values and are essential for conditional programming. Factors, another important data type in R, are used for categorical data and can significantly impact the analysis and visualization of your data.\nUnderstanding and correctly identifying these data types ensures that you apply appropriate operations on them and that you choose the correct methods for analysis and visualization. Misunderstanding the type of data you are dealing with can lead to errors, incorrect analysis results, and misrepresentations of data.\nOne of the beautiful aspects of R is its flexibility and versatility, which is largely due to the variety of packages and methodologies it offers for carrying out similar tasks. Base R, the Tidyverse, and data.table each provide different ecosystems with unique sets of functions and syntax that can be used to achieve the same ends.\nWhile it may initially seem premature to delve into discussions about different ecosystems in our introductory programming chapter, there’s good reason for this approach. Introducing these concepts early on allows us to create a foundational understanding of the varied programming landscapes within R. This awareness aids in navigating the expansive world of R, recognizing the interconnectedness of various ecosystems, and making informed choices about the tools and approaches best suited for our work.\nFrankly, starting with traditional R syntax and slowly progressing towards more modern forms is not the most efficient learning pathway. Modern R ecosystems, such as the tidyverse, have been developed with a focus on user-friendly and intuitive syntax. They simplify the process of data analysis, visualization, and manipulation, effectively minimizing the frustrations and quirks that characterized earlier versions of R. By starting with these modern tools, you’re better equipped to navigate and utilize the power of R from the get-go.\nThere are various ecosystems within R that cater to different needs and approaches when it comes to data analysis. Among them, three stand out due to their extensive use and comprehensive capabilities - Base R, Tidyverse, and data.table. It’s worth noting that each of these ecosystems is self-sufficient, meaning you can conduct an entire analysis without needing to switch ecosystems. That being said, each ecosystem has its unique strengths and is best suited to different use cases, so it’s beneficial to be familiar with all three. We will delve into the specifics of these ecosystems in the following sections.\nUnderstanding these different R ecosystems can be likened to the experience of cooking in different types of kitchens with various sets of utensils and appliances.\nIn each case, you can achieve the same end result - a prepared meal (or in the case of R, a processed dataset). However, the method you choose depends on your specific requirements, the scale of your meal/data, and your personal preference or familiarity with the tools at hand."
  },
  {
    "objectID": "rprogramming.html#hands-on-experience-with-each-data-type",
    "href": "rprogramming.html#hands-on-experience-with-each-data-type",
    "title": "5  Getting started with R programming",
    "section": "9.1 Hands on experience with each data type",
    "text": "9.1 Hands on experience with each data type\n\nNumeric: Calculate the average body mass index (BMI) of a patient over the last 12 months.\n\nbmi_values &lt;- c(25.1, 26.3, 25.9, 25.5, 26.0, 26.4, 26.1, 25.8, 26.3, 26.2, 26.0, 26.1)     average_bmi &lt;- mean(bmi_values)     \nprint(average_bmi)\n\nInteger: Count the number of patient visits in a year.\n\nvisit_counts &lt;- c(1, 2, 1, 2, 2, 3, 2, 2, 1, 2, 2, 1)\ntotal_visits &lt;- sum(visit_counts)     \nprint(total_visits)\n\nComplex: Not typically used in medical contexts, but can be used for mathematical calculations involving complex numbers.\nCharacter: Store a patient’s name and print it.\n\npatient_name &lt;- \"John Doe\"     \nprint(patient_name)\n\nLogical: Check if a patient’s BMI is above 25 at each visit.\n\nbmi_above_25 &lt;- bmi_values &gt; 25     \nprint(bmi_above_25)\n\nFactor: Create a factor object for patient blood types and print the levels.\n\nblood_types &lt;- factor(c(\"A\", \"B\", \"AB\", \"O\", \"O\", \"A\", \"B\", \"AB\", \"O\", \"A\"))     \nprint(levels(blood_types))\n\nDate/Time: Create a date object for patient visits and calculate the duration since the first visit.\n\nvisit_dates &lt;- as.Date(c(\"2023-01-01\", \"2023-02-01\", \"2023-03-01\", \"2023-04-01\", \"2023-05-01\", \"2023-06-01\"))\nduration &lt;- Sys.Date() - min(visit_dates)\nprint(duration)\nPlease replace Sys.Date() with the actual date when running the exercise."
  },
  {
    "objectID": "rprogramming.html#additional-programming-concepts-raised-by-these-examples",
    "href": "rprogramming.html#additional-programming-concepts-raised-by-these-examples",
    "title": "5  Getting started with R programming",
    "section": "9.2 additional programming concepts raised by these examples",
    "text": "9.2 additional programming concepts raised by these examples\nAbsolutely, these examples do bring up some important programming concepts used in R:\n\nThe c function: In R, c() stands for ‘concatenate’ or ‘combine.’ It is used to combine values into a vector. For example, in bmi_values &lt;- c(25.1, 26.3, 25.9, 25.5), we are using c() to combine the BMI values into a numeric vector.\nQuotes: In R, both single (') and double (\") quotes can denote character strings. For example, \"John Doe\" is a character string. It’s important to start and end a string with the same type of quote.\nOperators: These examples use several types of operators. The &lt;- symbol is the assignment operator, used to assign values to variables (e.g., bmi_values &lt;- c(25.1, 26.3, 25.9, 25.5)). The &gt; symbol is a comparison operator, used to compare values (e.g., bmi_values &gt; 25 checks if each value in bmi_values is greater than 25). The + and - symbols are arithmetic operators, used for mathematical calculations.\nAssignment: In R, we use the &lt;- operator to assign values to variables. A variable is a storage place for values that you want to use or manipulate. For example, bmi_values &lt;- c(25.1, 26.3, 25.9, 25.5) creates a variable named bmi_values and assigns it the values specified in the c() function.\nFunctions: Functions are pieces of code that perform a specific task. R has many built-in functions (like c(), mean(), sum(), print(), as.Date(), min()) that take inputs (arguments), perform certain computations, and return outputs. Functions are invoked by their name followed by parentheses containing the arguments.\nLogical values: The output of the comparison operation bmi_values &gt; 25 is a logical vector that indicates whether each BMI value is greater than 25. Logical vectors are crucial for performing operations based on certain conditions.\n\nThese are some of the basic programming concepts in R. As you progress, you will encounter more advanced concepts, but these form the foundation of programming in R.\nVectors and lists are two fundamental data structures in R, and they’re related to the concept of data types.\n\nVectors: A vector in R is a sequence of data elements of the same basic type. Members in a vector are officially called components. All components of a vector must be of the same type. Hence, we have numeric, logical, character, and complex vectors.\nExample of a numeric vector:\npatient_ages &lt;- c(25, 30, 35, 40)\nLists: A list in R, however, is an ordered collection of objects, where these objects can be of different types (numeric, character, logical, etc.). This is a major difference from vectors. In other words, a list in R can contain numbers, strings, vectors, other lists, and so on.\nExample of a list:\npatient_info &lt;- list(name = \"John Doe\", age = 35, disease = \"Hypertension\", visit_dates = as.Date(c(\"2023-01-01\", \"2023-02-01\")))\nIn the context of data types, vectors and lists are containers that hold data. However, vectors can hold only one type of data at a time (all numeric, all character, etc.), while lists can hold multiple types of data at once (numeric, character, other lists, vectors, etc.).\n\nThis flexibility makes lists particularly useful for complex data manipulation tasks and for creating data frames (which are a specific type of list where each element is a vector of equal length - think of them like a spreadsheet with columns of different types of data, but the same number of rows in each column)."
  },
  {
    "objectID": "rprogramming.html#exercise-lists-and-vectors",
    "href": "rprogramming.html#exercise-lists-and-vectors",
    "title": "5  Getting started with R programming",
    "section": "9.3 exercise lists and vectors",
    "text": "9.3 exercise lists and vectors\n\nVectors:\nImagine you’re a doctor and you have several patients. You could create a vector for each type of patient information. For example, you might have a vector of patient ages, another vector for patient names, and so on. Each vector contains only one type of information (either all numbers, all characters, etc.).\npatient_names &lt;- c(\"John Doe\", \"Jane Doe\", \"Mary Johnson\", \"James Smith\")\npatient_ages &lt;- c(25, 30, 35, 40)\nHere, patient_names is a character vector containing names of patients, and patient_ages is a numeric vector containing the ages of the patients. All the data elements in each vector are of the same type.\nLists:\nNow imagine that you want to keep all the information related to each patient together. In other words, you want a structure that contains the name, age, and perhaps even the medical history for each patient. This is where lists come in.\njohn_doe_info &lt;- list(name = \"John Doe\", age = 25, diseases = c(\"Hypertension\", \"Diabetes\"))\njane_doe_info &lt;- list(name = \"Jane Doe\", age = 30, diseases = c(\"Asthma\"))\nHere, john_doe_info and jane_doe_info are lists that contain different types of data. For each patient, we have a character string (name), a numeric value (age), and a character vector (diseases).\nPractice Exercise:\nNow that you’ve seen these examples, try creating a vector and a list for a new patient. The vector should contain the patient’s systolic blood pressure measurements from their last three visits, and the list should contain the patient’s name, age, and diseases.\n\nRemember, vectors are great for handling sequences of data of the same type, while lists allow you to group together related data of different types. Understanding when to use each one is a key part of becoming proficient in R."
  },
  {
    "objectID": "redcap1.html#introduction-overview-of-redcap-database-software",
    "href": "redcap1.html#introduction-overview-of-redcap-database-software",
    "title": "6  Using REDCap Data",
    "section": "6.1 Introduction: Overview of REDCap Database Software",
    "text": "6.1 Introduction: Overview of REDCap Database Software\nREDCap (Research Electronic Data Capture) is a secure web application designed for building and managing online databases for clinical research. REDCap provides an intuitive interface for validated data capture, audit trails for tracking data manipulation and export procedures, automated export procedures for seamless data downloads to common statistical packages, and procedures for data integration and interoperability with external sources. REDCap databases comply with 21 CFR Part 11, FISMA, HIPAA, and GDPR regulations, ensuring the highest data security standards. The software has been broadly adopted by over 950 academic institutions and clinical partners across the globe to support translational research.\nFor physicians working in clinical research, managing and analyzing data collected in REDCap will invariably become necessary. REDCap stores longitudinal, multi-arm study data in a complex structure that can be challenging to wrangle. Accessing the data via periodic Excel exports is time-consuming and restricts the analyses that can be performed."
  },
  {
    "objectID": "redcap1.html#what-we-will-learn",
    "href": "redcap1.html#what-we-will-learn",
    "title": "6  Using REDCap Data",
    "section": "6.2 What we will learn",
    "text": "6.2 What we will learn\nIn this chapter we will learn how to wrangle REDCap output into analysis-ready formats. In clinical research settings, physicians often receive REDCap data as periodic export files for analysis. This exported data retains the intricate nested structure that can make wrangling in REDCap challenging."
  },
  {
    "objectID": "redcap1.html#introduction-to-the-datasets",
    "href": "redcap1.html#introduction-to-the-datasets",
    "title": "6  Using REDCap Data",
    "section": "6.3 Introduction to the datasets",
    "text": "6.3 Introduction to the datasets\nWe will be working with a deidentified dataset for a small open-label pilot of transcranial magnetic stimulation treatment for depression. The trial is registered at NCT05271357 and ran between 11/2021 and 3/2023 to prepare for a sham-controlled randomized controlled trial (RCT)."
  },
  {
    "objectID": "redcap1.html#introduction-to-the-dataset",
    "href": "redcap1.html#introduction-to-the-dataset",
    "title": "6  Using REDCap Data",
    "section": "6.4 Introduction to the Dataset",
    "text": "6.4 Introduction to the Dataset\nWe will be working with a deidentified dataset for a small open-label pilot of transcranial magnetic stimulation treatment for depression. The trial is registered at NCT05271357 and ran between 11/2021 and 3/2023 to prepare for a sham-controlled randomized controlled trial (RCT).\nWe will work through the real-world challenge of starting with raw exported data from a REDCap database to generate outputs for a short manuscript. This reflects a common workflow you may encounter when leveraging REDCap and R to efficiently analyze real clinical research data."
  },
  {
    "objectID": "redcap1.html#overview-of-the-entire-process",
    "href": "redcap1.html#overview-of-the-entire-process",
    "title": "6  Using REDCap Data",
    "section": "6.5 Overview of the entire process",
    "text": "6.5 Overview of the entire process\nLet’s break up our task into universal tasks:\n\nImport the RAW REDCap data into R\nExamine the structure of the extracted data frames\nUse hypotheses to create data frames for analysis\nConduct statistical analyses\nReport the results using tables and graphs"
  },
  {
    "objectID": "redcap1.html#import-the-raw-redcap-data-into-r",
    "href": "redcap1.html#import-the-raw-redcap-data-into-r",
    "title": "6  Using REDCap Data",
    "section": "6.7 Import the RAW REDCap data into R",
    "text": "6.7 Import the RAW REDCap data into R\nExport functions can be accessed for a database through Applications sidebar and the Data Exports, Reports, and Stats link.\n\n\n\nREDCap Export (v. 13.7.6)\n\n\nThere are several ways to get data out of REDCap.\n\nExport the data as a CSV or Excel file\nExport data into R format\nAccess data using API key\n\n\n6.7.1 Export the data as a CSV or Excel file\nThe most common is to export the data as a CSV or Excel file. This is most likely what you will receive from a research assistant or collaborator.\n\n\n6.7.2 Export data into R format\nRequesting the data in R format preserves the structure and types of data when importing into R. This is the best option if you have access to the REDCap database and want to import the data directly into R.\n\n\n6.7.3 Access data using API key\nWe will learn more about this option later in the chapter to avoid unnecessary complexity. This is the best option if you want to access the data in real-time and have access to the REDCap database."
  },
  {
    "objectID": "redcap1.html#the-basic-structure-of-redcap-data",
    "href": "redcap1.html#the-basic-structure-of-redcap-data",
    "title": "6  Using REDCap Data",
    "section": "6.3 The basic structure of REDCap data",
    "text": "6.3 The basic structure of REDCap data\nREDCap databases are composed of multiple components:\n\nMetadata Dictionary - This defines the structure of the database including all fields, forms, instruments, and events. The dictionary provides details like variable names, field types, validation rules, etc.\nRecords - These contain the actual data values entered for each participant across all forms and events. Records are identified by a unique record ID.\nInstruments - These are groups of related data fields focused on collecting a certain type of data, like demographics or lab results.\nEvents - Events group related instruments that are completed at certain defined timepoints, like enrollment, follow-ups, final visit etc.\nForms - Forms display instruments together for data entry in the REDCap interface. They can mix instruments from different events.\nField Types - Each field has a specific type like text box, checkbox, dropdown, date, etc. Used to structure data entry.\nReports and Exports - REDCap provides tools for exporting and reporting on the data in different formats like CSVs, SAS, SPSS, R, APIs etc."
  },
  {
    "objectID": "redcap1.html#protecting-participant-privacy",
    "href": "redcap1.html#protecting-participant-privacy",
    "title": "6  Using REDCap Data",
    "section": "6.6 Protecting participant privacy",
    "text": "6.6 Protecting participant privacy\nWhen exporting data from REDCap, it is important to properly de-identify any protected health information (PHI). In this example, we will utilize the “Remove All Identifier Fields” option during the data export process. This ensures that any PHI variables that were pre-configured as identifiers in the REDCap data dictionary will be removed from the exported dataset. Using this export option is an easy way to protect participant privacy when working with REDCap data outside of the secure database."
  },
  {
    "objectID": "redcap1.html#importing-the-data-into-r",
    "href": "redcap1.html#importing-the-data-into-r",
    "title": "6  Using REDCap Data",
    "section": "6.10 Importing the data into R",
    "text": "6.10 Importing the data into R\nUsing REDCap data with R is handled through a variety of user-generated packages and scripts. In my experience, no single one package will work for all situations.\nThe currently available packages for working with REDCap assume you will be accessing the data via API request rather than a manual export. Let’s instead use a workflow to import the manual data export either in CSV or R format. Luckily, R is very adept at easily incorporating custom functions from online sources.\n\n6.10.1 The Source Function: Activating a Function from a Web Link\nOne powerful feature in R is the ability to access and run code from a URL using the source() function. This allows easy use of shared code without needing to manually download scripts.\nFor example, Frank Harrell, Jr, PhD, a professor of biostatistics at Vanderbilt University, maintains an R code repository on GitHub. Among his contributions is a custom function for importing REDCap data called importREDCap().\nRather than copy-pasting this code, we can run it directly from his GitHub repository using source() function. However, the link passed to source() must be the raw code itself, not the formatted webpage. To get the correct URL, navigate to the code file on GitHub. Then click the “Raw” button on the upper right of the file view. This will change the URL to point to the raw code which can then be used in source().\n\n\n\nRaw Source\n\n\nFor example, here is the URL for the raw importREDCap.r code:\nhttps://raw.githubusercontent.com/harrelfe/rscripts/master/importREDCap.r\nThen, to activate the function in your R session, simply pass the URL to the source() function:\n# Source the importREDCap function \nsource(\"https://raw.githubusercontent.com/harrelfe/rscripts/master/importREDCap.r\")"
  },
  {
    "objectID": "redcap1.html#using-data-from-redcap-in-r",
    "href": "redcap1.html#using-data-from-redcap-in-r",
    "title": "6  Using REDCap Data",
    "section": "6.7 Using Data from REDCap in R",
    "text": "6.7 Using Data from REDCap in R\nExport functions can be accessed for a database through Applications sidebar and the Data Exports, Reports, and Stats link. In addition, many data functions also require the data dictionary to be exported. This can be done through the Project Home and Design sidebar and the Dictionary link.\n\n\n\n\n\n\nNote\n\n\n\nIf you are accessing the data directly from the REDCap database and do not see these options, it is most likely a permissions issue. For example, you will need to request the data dictionary access or API access from the database administrator.\n\n\n\n\n\nREDCap Export Options (v. 13.7.6)\n\n\nThough R offers several data export options, they are not equally compatible with downstream workflows. For instance, exporting data as a Microsoft Excel file enables quick viewing but requires tedious manual processing to extract individual data tables. Using the API to access data directly from the REDCap database is the most efficient option but requires special access and additional programming skills.\nTo most efficiently work with REDCap data in R using our recommend workflow, we recommend requesting the data as follows:\n\nExport the data in R format (includes a R script and associated CSV file)\nExport the data dictionary in CSV format\n\n\n\n\n\n\n\nNote\n\n\n\nCSV files are a common format for storing data in a tabular format. CSV stands for comma-separated values. Each row in the file corresponds to a row in the table. Each column in the file corresponds to a column in the table. The values in each row are separated by commas. CSV files can be opened in Excel or other spreadsheet programs."
  },
  {
    "objectID": "redcap1.html#why-is-redcap-data-so-difficult-to-work-with",
    "href": "redcap1.html#why-is-redcap-data-so-difficult-to-work-with",
    "title": "6  Using REDCap Data",
    "section": "6.8 Why is REDCap data so difficult to work with?",
    "text": "6.8 Why is REDCap data so difficult to work with?\nREDCap data can be challenging to work with for analysis due to its nested structure. Though the full study database can be downloaded as one file, this concatenates data from different instruments and timepoints into a massive single table. Visually, this table has many blank rows and columns as a result. While convenient for export, this concatenated format requires careful reorganization to parse out individual instruments and events cleanly for analysis.\nThough REDCap instruments, such as multiple choice fields, are well suited for user entry, the exported data requires additional processing to be analysis-ready. For example, multiple choice fields are exported as a series of 0/1 indicator variables rather than a single field. This requires condensing the variables into a consolidated variable for analysis. In addition, the labels for each option are not retained in the exported data. Instead, the column names are formatted as “questionname___1”. This makes it difficult to know what each column represents without referencing the data dictionary.\nREDCap’s longitudinal data collection allows multiple timepoint records per participant, enabling complex study designs. However, this results in “long” format data during export, with each response in a separate row, rather than a “wide” format with timepoints as columns. This requires reshaping the data before analysis. In addition, dates and timestamps may not appear aligned properly between records during export.\nLuckily, the user community in R has developed several packages to help with these challenges. We will leverage these packages to efficiently wrangle REDCap data into analysis-ready formats."
  },
  {
    "objectID": "redcap1.html#orientation-to-the-redcap-data-export",
    "href": "redcap1.html#orientation-to-the-redcap-data-export",
    "title": "6  Using REDCap Data",
    "section": "6.9 Orientation to the REDCap Data Export",
    "text": "6.9 Orientation to the REDCap Data Export\nGiven the various designs of REDCap databases, we recommend scheduling a brief consultation with your REDCap database administrator to walk through the original database design, including what REDCap options and modules have been implemented. Though some of this information can be inferred from the exported data, having a clear undestanding of what to expect will save time and frustration."
  },
  {
    "objectID": "redcap1.html#an-overview-of-key-redcap-concepts",
    "href": "redcap1.html#an-overview-of-key-redcap-concepts",
    "title": "6  Using REDCap Data",
    "section": "6.3 An Overview of Key REDCap Concepts",
    "text": "6.3 An Overview of Key REDCap Concepts\nREDCap databases are composed of multiple components: * Project - A research study. * Arms - Group study events into sequences. * Metadata Dictionary - This defines the structure of the database including all fields, forms, instruments, and events. The dictionary provides details like variable names, field types, validation rules, etc. * Records - These contain the actual data values entered for each participant across all forms and events. Records are identified by a unique record ID. * Instruments - These are groups of related data fields focused on collecting a certain type of data, like demographics or lab results. * Longitudinal module - This is a project-level setting that must be enabled to allow multiple records per subject over time. It unlocks functionality needed for longitudinal projects. * Longitudinal/repeating instruments - These are individual data collection forms that can be completed multiple times. They can be used in both classic and longitudinal projects. * Events - Events group related instruments that are completed at certain defined timepoints, like enrollment, follow-ups, final visit etc. * Forms - Forms display instruments together for data entry in the REDCap interface. They can mix instruments from different events. * Field Types - Each field has a specific type like text box, checkbox, dropdown, date, etc. Used to structure data entry. * Reports and Exports - REDCap provides tools for exporting and reporting on the data in different formats like CSVs, SAS, SPSS, R, APIs etc. * REDCap Shared Library - Public repository of instruments.\n\n\n\n\n\n\nDr. Smith’s Example REDCap Project\n\n\n\n\n\n\nDr. Smith’s Hypothetical Nutrition Study\n\n\nLet’s organize these terms into a hypothetical story to place them into context:\nDr. Smith is leading a research study on the effects of a new diet regimen on heart health. In REDCap, this study is known as a Project. Given the nature of her study, she decides to break it down into multiple phases, or Arms, each representing a distinct sequence in her research.\nIn the first phase, she gathers baseline health metrics. In the subsequent phases, participants undergo different diet regimens and periodic health check-ups. Each of these phases requires participants to fill out various Instruments, which are essentially data collection forms capturing vital information about their health, diet habits, and more.\nBefore she starts collecting data, Dr. Smith defines her Metadata, a data dictionary that describes all the fields she’ll use in her project. This ensures consistency and clarity for data entry. She also explores the REDCap Shared Library, a public repository, and discovers some pre-existing Instruments that can be beneficial for her study.\nAs participants enroll, each one’s data is stored as a Record. REDCap doesn’t have an explicit concept for a “subject”, but in practice, a Record essentially represents data for one subject or participant.\nWhile Dr. Smith’s study inherently possesses a Longitudinal nature (collecting data at multiple timepoints for each participant), she decides not to employ REDCap’s Longitudinal Module. Instead, she organizes her data using a combination of carefully crafted Instruments and manual record-keeping. For instance, some Instruments, like a daily diet log, are Repeating Instruments and can be filled out multiple times by the participants throughout the study. Certain collections of Instruments are grouped and completed together, like during a monthly health check-up. In REDCap, these are called Events. Since Dr. Smith’s study is longitudinal, it requires the use of such Events.\nHad Dr. Smith been running a simpler study, capturing data only once per participant, it would be termed as a Classic Project in REDCap."
  },
  {
    "objectID": "redcap1.html",
    "href": "redcap1.html",
    "title": "6  Using REDCap Data",
    "section": "",
    "text": "7 Stage 1: Importing the Data\nTo begin a new Quarto analysis document for this project:\nThis will generate a template Quarto file to work from. The template contains example code, text, and formatting to illustrate Quarto features.\nThe screen shot above highlights a few key points:\nLet’s work through our main outline step-by-step and use a suboutline to further break down the steps needed to complete each section.\nOur suboutline is as follows:"
  },
  {
    "objectID": "redcap1.html#why-is-exported-redcap-data-so-difficult-to-work-with",
    "href": "redcap1.html#why-is-exported-redcap-data-so-difficult-to-work-with",
    "title": "6  Using REDCap Data",
    "section": "6.8 Why is Exported REDCap data so difficult to work with?",
    "text": "6.8 Why is Exported REDCap data so difficult to work with?\nREDCap data can be challenging to work with for analysis due to its nested structure. Though the full study database can be downloaded as one file, this concatenates data from different instruments and timepoints into a massive single table. Visually, this table has many blank rows and columns as a result. While convenient for export, this concatenated format requires careful reorganization to parse out individual instruments and events cleanly for analysis.\nThough REDCap instruments, such as multiple choice fields, are well suited for user entry, the exported data requires additional processing to be analysis-ready. For example, multiple choice fields are exported as a series of 0/1 indicator variables rather than a single field. This requires condensing the variables into a consolidated variable for analysis. In addition, the labels for each option are not retained in the exported data. Instead, the column names are formatted as “questionname___1”. This makes it difficult to know what each column represents without referencing the data dictionary.\nREDCap’s longitudinal data collection allows multiple timepoint records per participant, enabling complex study designs. However, this results in “long” format data during export, with each response in a separate row, rather than a “wide” format with timepoints as columns. This requires reshaping the data before analysis. In addition, dates and timestamps may not appear aligned properly between records during export.\nLuckily, the user community in R has developed several packages to help with these challenges. We will leverage these packages to efficiently wrangle REDCap data into analysis-ready formats."
  },
  {
    "objectID": "redcap1.html#redcap-tools",
    "href": "redcap1.html#redcap-tools",
    "title": "6  Using REDCap Data",
    "section": "6.11 REDCap tools",
    "text": "6.11 REDCap tools\nSearching for REDCap tools to use with R will yield a variety of options. However, to in this workflow we will use a lesser known but highly efficient package created Paul Egler (Spectrum Health Research) known as REDCap Repeating Instrument Table Splitter, aka REDCapRITS.\nThere are several highly sophisticated packages for working with REDCap data in R including tidyREDCap and REDCapR. These packages are primarily designed to work with the REDCap API and require special access to the database. Though we will not use these functions for our primary data wrangling, we can selectively use functions from these packages to assist in formating complex instruments.\nlibrary(tidyverse)\nlibrary(REDCapRITS)"
  },
  {
    "objectID": "redcap1.html#editing-the-quarto-template",
    "href": "redcap1.html#editing-the-quarto-template",
    "title": "6  Using REDCap Data",
    "section": "7.1 Editing the Quarto Template",
    "text": "7.1 Editing the Quarto Template\nThe Quarto template consists of a YAML header and a Markdown body. The YAML header contains metadata about the document, including the title, author, and output format. The Markdown body contains the text and code for the document. We will use this template to create our analysis document.\n\n7.1.1 Editing the YAML Header\nFor now, we will leave the YAML header as is. In the future, we can modify the header to change aspects of the entire document, such as changing the export format to PDF or Word.\n\n\n7.1.2 Editing the Markdown body\nThe document outside the YAML header is written in Markdown, a simple text formatting language. When we write code, we specify “blocks” using three backticks (```) and {r} to specify the programming language.\n\n\n\n\n\n\nQuarto: for Scientific and Technical publishing\n\n\n\nQuarto makes it simple to integrate prose, code, and results seamlessly in a single document. By keeping everything together, Quarto streamlines the process of creating reproducible data analysis reports. Quarto itself is extensively documented and we would recommend reviewing the user guide.\n\n\n\n\n7.1.3 Adding an outline of proposed analyses\nBefore we begin writing code, let’s add an outline of the analyses we plan to perform. This will help us organize our thoughts and plan our code.\nIn the Markdown body, add the following text:\nHere is an outline for the analysis document:\n\n# Overview\n\nAnalysis of a small open-label pilot of transcranial magnetic stimulation treatment for depression. The trial is registered at NCT05271357 and ran between 11/2021 and 3/2023 to prepare for a sham-controlled randomized controlled trial (RCT). \n\n## 1. Setup environment\n\n- Load R packages\n- Set options\n\n## 2. Load data\n\n- Read in dataset \n- Inspect data\n\n## 3. Demographics table\n\n- Make table of demographics\n\n## 4. Baseline characteristics table \n\n- Make table of baseline scores\n\n## 5. Treatment effects\n\n- Statistical tests on primary outcome\n- Statistical tests on secondary outcomes\n\n## 6. Visualizations\n\n- Graph changes in primary outcome\n- Graph changes in secondary outcomes\n\nThe outline covers the key sections needed - importing data, making summary tables, analyzing treatment effects, and creating visualizations. Each section is briefly described to provide a sense of the planned content. The outline can serve as a guide for drafting the full report."
  },
  {
    "objectID": "redcap1.html#quarto-as-a-dynamic-document",
    "href": "redcap1.html#quarto-as-a-dynamic-document",
    "title": "6  Using REDCap Data",
    "section": "8.1 Quarto as a dynamic document",
    "text": "8.1 Quarto as a dynamic document\nTo this point, we have been writing in Markdown which is best suited for formatted text. In many ways this is similar to how we would write a document in Microsoft Word or Google Docs.\nHowever, when we use Quarto with R code, we create a “living document” that can be executed and updated. This allows us to write code and display the results in the same document. If written correctly, the document can be re-run at any time to update the results.\nQuarto components at a glance: 1. YAML header: metadata about the document 2. Markdown body: formatted text 3. Code blocks: executable code 4. Output: results of executed code\n\n\n\n\n\n\nManaging Quarto Output\n\n\n\nIn a Quarto document, the content you directly write in Markdown represents the body of the report. This includes text, images, and formatting. The output produced by executing code blocks, however, depends entirely on the code itself.\nAs the author, you have full control over the Markdown body. But the output from chunks is determined by the underlying R code. So running chunks may generate results like tables, plots, and printed text that can disrupt your intended document layout.\nFortunately, Quarto gives you many options to manage code chunk output. You can direct where results appear, prevent code from rendering entirely, save results as variables, and more. Using these options skillfully is key to creating Quarto reports with a polished, publication-quality look. The code results enrich the document without disrupting flow.\nHere is a Markdown table summarizing options for managing code chunk output in Quarto:\n\n\n\nOption\nDescription\n\n\n\n\necho = FALSE\nPrevents code from rendering\n\n\ninclude = FALSE\nPrevents output from rendering\n\n\nmessage = FALSE\nSuppresses printed messages\n\n\nwarning = FALSE\nSuppresses warnings\n\n\nfig.show = \"hide\"\nHides plots\n\n\nresults = \"hide\"\nShows code but not text output\n\n\nstore_result= \"data\"\nSaves output as object\n\n\ninsert\nInserts output into Markdown\n\n\nR Markdown blocks\nFor custom output formatting\n\n\noutput-location\nDirects output location\n\n\nQuarto hooks\nModify output with R code\n\n\n\nThis covers some common options for controlling if and how chunk output is rendered in the final document. Using these gives you fine-grained control over output.\n\n\n\n8.1.1 Quarto: Modifications to default settings\nBy default, a Quarto document will display executed code within the document. To simplify this tutorial, we will modify the default setting to display executed code into the console window. This will allow us to clearly separate the code and the output.\n\n\n\nQuarto default output"
  },
  {
    "objectID": "redcap1.html#adding-an-r-code-blocks",
    "href": "redcap1.html#adding-an-r-code-blocks",
    "title": "6  Using REDCap Data",
    "section": "8.2 Adding an R code blocks",
    "text": "8.2 Adding an R code blocks\nQuarto neatly separates code from the formatted Markdown text by using code blocks. Code blocks are sections of code that are executed and the results are displayed in the document.\nTo add a code block, we use three backticks (```) and {r} to specify the programming language. For example, to add a code block for R, we would write:\n\n## 1. Setup environment\n\n# 1. load general data science packages\n\nlibrary(tidyverse)\n\n# install REDCapRITS to easily wrangle REDCap data\n# https://github.com/SpectrumHealthResearch/REDCapRITS\n\nif (!require(devtools)) install.packages(\"devtools\")\ndevtools::install_github(\"SpectrumHealthResearch/REDCapRITS/R\")\n\n# load REDCapRITS library\n\nlibrary(REDCapRITS)\n\nLet’s walk through this code:\nHere is one way to explain that code snippet in a standardized format:"
  },
  {
    "objectID": "redcap1.html#load-packages",
    "href": "redcap1.html#load-packages",
    "title": "6  Using REDCap Data",
    "section": "8.3 Load Packages",
    "text": "8.3 Load Packages\nThis code chunk loads the required R packages for the analysis. The output: false option prevents the code itself from rendering in the final document.\n\n8.3.1 Tidyverse\nlibrary(tidyverse)\nThe tidyverse package contains core tidy data manipulation packages like dplyr, tidyr, readr, and ggplot2.\n\n\n8.3.2 REDCapRITS\nif (!require(devtools)) install.packages(\"devtools\")\ndevtools::install_github(\"SpectrumHealthResearch/REDCapRITS/R\")\nREDCapRITS provides functions to easily wrangle and analyze data exported from REDCap. If it is not already installed, this code will install it from GitHub using devtools.\nlibrary(REDCapRITS)\nLoad the REDCapRITS package."
  }
]